* 机器学习
  * 分类
    * 逻辑回归
    * SVM
    * 随机森林
    * 朴素贝叶斯
    * DL
    * DT
  * 回归
    * 线性回归
    * 普通最小二乘法
    * 逐步回归
    * 多元自适应回归样条
  * 聚类
    * k均值
    * 基于密度聚类
    * LDA
  * 推荐系统
    * 协同过滤算法
    * FM
  * 融合、提升
    * Bagging
    * Adaboost
    * GBDT
    * GBRT
    * Stacking
    * Blending
    * XGBoost
    * EM算法
  * 框架
    * scikit-learn
    * libsvm
    * keras/tensorflow



* * ML步骤
    * 数据预处理
    * 特征提取/特征选择
      * 常用方法：PCA、LDA
        * PCA（主成分分析）
        * LDA（线性判别分析）
    * 预测



表示（特征）学习：一种算法可以自动地学习 输入转换为有效的特征，能够提高模型的性能。

语义鸿沟（Semantic Gap) 数据的底层特征和高层语义之间的不一致 、差异性。

> ​	比如“🚗”的底层是形状、颜色等像素特征，但人理解是高层特征（这是辆车）。
>
> 如果一个预测模型是建立在底层特征上，就会导致我们对这个模型的预测能力要求过高。因为建立在颜色形状等特征和 是不是“车” 没有联系。

什么是好的表示？怎么学到好的表示？

表示可分为

* 局部表示

  * 离散、符号表示。

    * One-Hot编码

      * 优点：简单直观，易于人工归纳和总结特征，并进行性特征组合。

        > ​	向量的特定维度上的值的意义是明确的。

        对于线性模型，计算效率很高

        > ​	向量的维度固定，可以采取一些优化计算的方法

      * 缺点：向量的维度太高，计算量大，拓展不容易

        不能体现不同编码之间的相似度（比如红色和中国红，红色和黑色）之间的差距是一样的。

* 分布表示（如颜色的RGB表示

  * 优点：向量维度低

    表示能力强，不同编码之间的相似度容易计算/体现

嵌入（Embedding)：把**高维**的**局部**的表示空间   映射到   一个非常低维的分布式的表示空间

好的高层语义表示 <=   从底层特征开始，经过多步 非线性转换 （也就是所谓 的深度）

> ​	结构越深，可以增加对特征的重用性。比如识别红色的五角星  ？？？



（一般特征学习 是和  预测模型  分开的）



深度学习 ：直接从原始数据出发，避免了特征（人工地）特征工程，把原始数据转换为高层次、高抽象的表示

* 主要需要解决贡献度分配问题（CAP），也就是不同的组件/参数对最终结果的影响。而NN的误差反向传播算法恰恰可以解决这个问题；

目前，DL主要采用NN模型，是因为NN的误差反向传播算法，可以比较好的解决CAP问题





端到端的学习：传统ML是把一个任务划为多个子模块，每个子模块分开来优化设计，这样容易把错误传播开来，而且每个子模块的优化目标和最终目标也不能保证其一致性。

端到端就是直接优化总目标。大部分采用NN的DL模型可以看作一种端到端的学习。



![image-20210323221410442](C:\Users\LLQ\AppData\Roaming\Typora\typora-user-images\image-20210323221410442.png)



深度学习实现的难点：因为一般都是采用误差反向传播算法来进行参数学习，所以会有大量的梯度计算。需要在CPU（执行指令）和GPU（数据的计算）之间切换



----

对于一些特定的任务（比如识别物体），人不知道自己如何做到的情况下，就没办法编一个计算机程序来完成同样的任务。现在ML的做法是通过大量的数据样本，来设计一个预测函数，来拟合这批数据。（具有相同高层语义的数据服从一样的分布）。

拟合的用的函数，我们叫做假设空间，$F(x,\theta)$，是一个函数族，分为线性和非线性。

>
>
>距离的特点：
>
>* 对称性
>* 三角形特性

损失函数

二分类问题的Hinge损失函数： Loss = max(0, 1-y*f) ,其中 y = {-1, +1};

风险最小化准则

* 经验风险：就是把每个样本的损失求个平均，求使期望值（大数定律）最小的参数。

* 过拟合产生的原因：训练集是真实集的一个很小的子集，而且包含一些噪声数据，很难反应全部数据的真实分布。

  * 解决方案（结构风险最小化）：参数正则化   $\lambda||\theta||^p$ ？？？？？？？？？？

    * L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。（L0是计算非零元素的个数）

    ![image-20210324135814967](C:\Users\LLQ\AppData\Roaming\Typora\typora-user-images\image-20210324135814967.png)

    * L2范数，使得每个参数接近0，使得很多参数的值很小，说明模型越简单， 也就是忽略的参数越多，相当于减少参数个数。

      

    * 提前停止训练，做法是：

      增加一个验证集：每次迭代后，在验证集上进行测试，但不使用验证集来矫正模型参数，看看在验证集上的错误率是否不在下降，然后停止。



优化问题

* 难题：局部最小值，ill-condition（参数轻微的变动，输出的变化就很大）

* 分类：参数优化、超参数优化

  * 超参数优化

    * 简单的方法是：搜索一组超参数区间不断试错调整

  * 参数优化

    凸函数有一套成熟高效的方法，所以尽可能选择合适的模型和损失函数构造一个凸函数。很多模型的目标是非凸的（如NN）

    * 梯度下降：就是沿着导数方向减小参数的值。导数>0，表示递增就减去一个数，否则就加上一个数。
      * 在整个训练集上的风险函数进行梯度下降，叫做BSD(批量梯度下降)。这样计算量太大，空间复杂度也高。
      * 因为训练集本身就是真实集的子集，所以为了匹配当前的算力，可以采用SGD，随机采集一个样本，计算这个样本的损失函数的梯度，并更新参数。而且这样还引入了随机噪声，更容易逃离局部最优点。 
      * 为了充分利用计算机的并行计算能力，可以考虑使用小批量梯度下降法（MBGD）折中。引入了一个超参数。

* 分类：

  * 监督学习

    * 回归：输出是连续值
    * 分类：输出是离散值
    * 结构化学习：输出是结构化的值，如图，序列、树等

  * 无监督学习：不含目标标签的训练样本中自动学习一些有价值的信息

    * 聚类
    * 密度估计
    * 特征学习
    * 降维

  * 强化学习（无输入/输出对，在线学习机制）

    * 通过交互来学习的算法，根据环境做出动作，然后得到一个（延时的）奖励//惩罚，然后不断调整策略。

    

回归问题：输出是连续值

* 线性回归

  * 最小二乘法

    解得 $W^*=(XX^T)^{-1}Xy$，如果不可逆，（矩阵求逆$O(n^3)$）

    * 常见的是因为样本数量$N$小于特征向量维度导致不可逆，可以采用PCA的方法，减少特征向量的维度
    * 或者就采用GD的方法求，也叫做最小均方算法（LMS）
    * 估计最优解，岭回归（Ridge Regression）：$W^*=(XX^T+\lambda I)^{-1}Xy$，使其满秩，即可逆。引入了超参数$\lambda$.对应的损失函数再加上$\frac{1}{2}\lambda||W||^2$项

  * 最大似然估计 MLE，引入一个服从$N(0, \sigma^2)$的偏置量。



特征表示：

* 图像
  * 像素矩阵、直方图、纹理特征、边缘特征
* 文本
  * BoW，词袋模型（Ont-hot编码），不考虑词序，不能表示文本信息。
  * 改进，N元特征(N-Gram)，以N个词为一个基本单元，再用BoW表示 ，随着N增长特征数量而指数级上升，十万百万级别

原始特征的不足：

* 特征单一、冗余度高、易变、噪声



**传统的方法，与预测模型的学习分离：**👇

特征选择

* 子集搜索，从$2^D$的候选子集中。常用的方法是采用贪心的策略：由空集合开始，每一轮添加该轮最优的特征，称为前向搜索（Forward Search）；或者从原始特征集合开始，每次删除最无用的特征，称为反向搜索（Backward Search）．
* 过滤式方法：每次增加有信息量的特征，删除没有信息量的特征。信息量通过信息增益（熵减小的程度=不确定性减小的程度）衡量
* 包裹式方法
* l1正则化导致特征稀疏，也就间接导致了特征选择

特征抽取：从一个特征空间，投影到另一个特征空间

* 有监督的，抽取对特定预测任务最有用的特征，如LDA,线性判别分析。
* 无监督的，减少冗余信息和噪声，如PCA、AE（自编码器）、独立成分分析。

特征选择/抽取可以降维。

端到端的模型，即DL，把表示学习和机器学习统一到一个模型中，避免不一致性。







**评价指标**

* 准确率
* 错误率
* 精确率（precision)，精度，查准率
  * $\frac{TP}{TP+FP}$，分母是模型认为的正例数量
* 召回率（Recall）,查全率
  * $\frac{TP}{TP+FN}$，分母是真实的正例数量
* F值：为精确率和召回率的调和平均
  * $\frac{(1+\beta^2)\times P \times R}{\beta^2\times P + R}$，通常取$\beta=1$称为F1值。



* 宏平均是每个类别的P和R的算术平均，更合理
* 微平均是每个样本的P（R和P的算术平均一样的）的算术平均

曲线

* AUC（Area Under Curve）？？？？
* ROC（Receiver Operating Characteristic）？？？？
* PR（Precision-Recall）？？？？

交叉验证可避免训练集和测试机的随机性对评价的影响

* 划分为K个不重复子集
* 每次选K-1个作为训练集，剩下一个作为测试集，得到一个错误率，进行K次
* 取K的模型的平均错误率

**理论**

PAC 可学习（Probably Approximately Correct-Learnable）的算法是指该学习算法能够在多项式时间内从合理数量的训练数据中学习到一个近似正确的𝑓(𝒙)．因为训练集是有限的。

没有免费午餐定理：对于基于迭代的最优化算法，不存在某种算法对所有问题（有限的搜索空间内）都有效。不能脱离具体问题来谈论算法的优劣

丑小鸭定理：不存在相似性的客观标准，一切相似性的标准都是主观的．

归纳偏置：对学习的问题做的一些假设（如，比如在最近邻分类器中，我们会假设在特征空间中，一个小的局部区域中的大部分样本同属一类．在朴素贝叶斯分类器中，我们会假设每个特征的条件概率是互相独立的。











**神经网络**

激活函数

* sigmoid，S型曲线，两端饱和，
  * logistic值域是[0, 1]
  * tanh 值域是[-1, 1]
  * hard-tanh 用分段线性模拟tanh
* (带参数的)relu  
  * 计算量小
* weakly-relu 
* Elu  $\gamma(exp(x)-1)$
* softplus，求导是logistic
* swich，$x\sigma(\beta x)$
* GELU $xP(X\leq x)$
* Maxout

前馈NN=多层logistic回归组成，

* 梯度计算
  * 数值计算（导数的定义），$O(N^2)$
  * 符号计算（化简）
  * 自动微分（计算图）

优化问题

* 非凸优化问题
* 梯度消失问题：误差从输出层进行反向传播时，每次都要乘以激活函数的导数，但有些激活函数如Sigmoid型函数（如logistic, tanh）的导数值域都小于等于1，且在0和1附近都趋于0.会使得误差不断衰减，甚至消失。或者层数太深，学习率大
  * 解决：采用导数大的函数如，relu
* 梯度爆炸
  * 初始化权值过大
  * 解决：设置阈值控制梯度大小

**CNN**

* 局部连接、参数共享、汇聚、与FNN相比参数更少

二维卷积

* 高斯滤波器：平滑去噪
* 提取边缘特征



* 步长（stride）
* 零填充（zero padding）
* 窄卷积：S=1，P=0
* 宽卷积：S=1，P=K-1
* 等宽卷积：S=1，P=(K-1)/2

输入神经元个数M，卷积大小K，步长S，两边填充P个0。该卷积层的神经元数量为(2P+M-K)/S+1



汇聚层：是进行特征选择，降低特征数量，从而减少参数数量

ResNet 残差网络：通过给卷积层增加直连边（残差连接）来提高信息的传播效率

**RNN** :短期记忆网络

* 增加延时器，记录历史活性值
* 自回归模型（AR），依赖于最近K次的输出，即最近K次自身的值
* 由外部输入的自回归模型（NARX）。依赖于最近K次的输入和输出

应用



* 序列到类别 （文本分类
  * 把（最后的/平均的）隐状态（一个）输入到分类器中
* 同步序列到序列 （词性标注
  * 把不同时刻的隐状态（多个）输入的分类器中
* 异步序列到序列，也叫做Encoder-Decoder  （机器翻译
  * 输出序列与输入序列不需要严格的对应关系/长度
  * Deocder常采用AR模型

采用梯度下降优化

* BPTT，随时间向后传播，计算量小，保存中间梯度，空间复杂度大
* RTRL，实时循环学习（前向传播），不需要梯度回传，适合在线学习或无限序列

长程依赖问题

* 因为梯度消失/爆炸，很难建立长时间间隔的状态之间的依赖关系，当t-K较大时，难建立长距离依赖
* 改进：权重衰减（给参数增加l1或l2范数的正则化项，限制参数的取值范围），梯度截断 解决 爆炸

门控机制

* 有选择的加入新的信息，有选择地以往之前累计的信息
* LSTM，长短期记忆网络
  * 记忆单元的更新公式
  * 隐状态的更新公式
  * 输入门、遗忘门、输出门的更新
* GRU 门控循环单元
  * 更新门、重置门
* 双向RNN（BiRNN）
* 递归RNN（RRNN）：句子的语义合成
  * 一般结构
  * 树状结构
  * 树状结构的LSTM
* GNN：知识图谱，社交网络

**网络优化**

* 非凸优化
  * 大部分驻点都是鞍点，局部最优点很少
  * 理想的最小值应该是平坦的，参数微小的变动不影响。NN中大部分局部最小值是等价的
  * 逃离鞍点
    * 优化算法
      * SGD
      * BGM
      * MBGD
        * 如何选取batch大小
          * 不影响SG的期望，但batch越大，SG的方差越小，也就是引入的噪声越小，训练也更稳定，可以设置比较大的lr（lr随着batch增大而增大）
          * 线性缩放原则（批量较小时适用），batch增加m倍，lr也增加m倍
        * 如何调整学习率
          * 学习率衰减
            * 分段常数衰减（根据迭代次数来）
            * 逆时衰减 $\alpha=\alpha_0 \frac{1}{1+\beta\times t}$ 刚开始下降的快
            * 指数衰减 $\alpha=\alpha_0 \beta^t$ 刚开始下降的快
            * 自然指数衰减 $\alpha=\alpha_0 exp(-\beta\times t)$ 刚开始下降的快
            * 余弦衰减 刚开始下降的慢 $\alpha=\frac{1}{2}\alpha_0 (1+cos(\frac{t \pi}{T}))$
          * 学习率预测，刚开始由于餐宿初始化等原因，梯度太大，训练不稳定，先以小lr迭代，（$α_t' = \frac{t}{T'}\alpha_0$）等稳定后在选择在一种学习率衰减方法
          * 周期性地调整lr（循环学习率），以便于逃离鞍点
          * 
        * 
          * AdaGrad算法
            * 根据不同参数的收敛情况分别设置学习率（通过计算每个参数梯度平方的累计值来决定学习率的大小 $\theta_t = -\frac{\alpha}{\sqrt{G_t+\epsilon}}\bigodot g_t$）,$G_t =\sum^t_1 g_t\bigodot g_t$
            * 缺点是，迭代一定次数找不到最优点，这时lr很小了，很难再继续找到最优点。
          * 改进->RMSprop
            * $G_t = \beta G_{t-1}+{(1-\ eta)g_t\bigodot g_t}$ 平方的指数衰减移动平均
          * 再改进->AdaDelta
            * $\theta_t = -\frac{\sqrt{\Delta X^2_{t-1}+\epsilon}}{\sqrt{G_t+\epsilon}}\bigodot g_t$
            * 其中$\Delta X^2_{t-1}=\beta \Delta X^2_{t-2}+{(1-\ \beta)\theta_{t-1}\bigodot \theta_{t-1}}$
            * 抑制了lr的波动
        * 如何修正梯度估计
    * 参数初始化方法
      * 预训练初始化：泛化
      * 随机初始化
      * 固定值初始化
        * 比如偏置向量初始化为0
        * 
      * 使用高斯分布、均匀分布对参数初始化，
    * 数据预处理
      * 归一化：把数据特征转换为相同的尺度
        * 最小最大值归一化
        * 标准化
        * 白化，降低特征之间的相关性，如PCA
        * 逐层归一化
          * 更好的尺度不变性，无论低层输入怎么办，高层输入相对稳定。
          * 使得大部分输入处于不饱和区域，也就梯度较大的区域，避免梯度消失
          * 使得NN的优化地形更平坦，梯度更稳定，允许更大的lr,提高收敛速度
          * 内部协变量偏移：输入分布变了，其参数需要重新学习，归一化使其分布稳定
          * 方法：
            * BN，批量归一化，不同样本之间求均值、方差，然后归一化
            * LN，层归一化，对一个中间层的所有神经元的进行归一化，单个样本的不同输出维度上求均值、方差，然后归一化
            * WN，权重归一化，再参数化
            * LRN，局部响应归一化和层归一化都是对同层的神经元进行归一化．不同的是，局部响应归一化应用在激活函数之后，只是对邻近的神经元进行局部归一化，并且不减去均值
            * 最大汇聚也具有侧抑制作用．但最大汇聚是对同一个特征映射中的邻近位置中的神经元进行抑制，而局部响应归一化是对同一个位置的邻近特征映射中的神经元进行抑制．
    * 更好的网络结构：ReLU、残差连接、逐层归一化
    * 超参数优化
      * 组合优化问题
        * 时间代价高
      * 网格搜索
        * 把超参数离散化，如lr = {0.01,  0.1,, 0.5, 1.0}
      * 随机搜索
        * 对超参数随机组成，避免不必要的尝试，取最好。因为有的超参数很重要（lr），有的不重要（正则化参数）
      * 贝叶斯优化
      * 动态资源分配
      * 神经架构搜索：通过NN自动设计网络架构（元学习、强化学习）
    * 网络正则化
      * l1在0处不可导，可用$\sum_{d=1}^D \sqrt{\theta^2_d+\epsilon}$ 近似
      * 权重衰减 $\theta_t=(1-\beta)\theta_{t-1}-\alpha g_t$
      * 验证集-提前停止
      * 丢弃法 dropout，设置一个丢弃率
        * 对输入层进行丢弃，相当于增加噪声
        * 每次都训练不同的网络参数，最终的NN相当于集成了指数级个不同网络的组合模型
        * RNN中，对非时间维度进行随机丢弃
    * 数据增强：增大数据量，避免过拟合
      * 主要用在图像上
        * 旋转、翻转、缩放、平移、加噪声
      * 对标签引入噪声，防止过拟合
        * Softmax 函数的性质可知，如果要使得某一类的输出概率接近于 1，其未归一化的得分需要远大于其他类的得分，可能会导致其权重越来越大，并导致过拟合
        * 简单的方法是 ，把one-hot中的1变为1-$\epsilon$，然后其他维度平均分配/或者根据标签特点分配

1 epoch = (N/K) * 1 iteration
<style class="fallback">body{visibility:hidden}</style><script>markdeepOptions={tocStyle:'long'};</script><!-- Markdeep: --><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>