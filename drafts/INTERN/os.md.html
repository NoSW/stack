OS & CA



**作用：**整合硬件资源集合：（处理器、内存、I/O模块，定时器，磁盘驱动器）网络资源（网络接口）为上层应用程序解决硬件的使用问题，并提供方便、安全、一致的接口，提高编程效率。

**内容包括**：

* 进程管理：创建、终止、切换、调度、IPC
* 内存管理：分配、交换、段页管理
* 文件系统
* IO管理：缓冲区管理、分配设备
* 支持功能：中断处理、记账、监视

**概念区分**

* 图灵机
  * 一种抽象的逻辑模型，一种模拟人类进行数学运算的机器。结构简单，功能完善
  * 图灵完备：可抽象成图灵机的系统、编程语言、或物理装置。
  * 一切可计算的问题，图灵机都就可以计算
  * 经典的停机问题是图灵不可解的（自我指涉，一阶逻辑的不完备性）
* 冯诺依曼结构
  * 程序指令存储器和数据存储器合并在一起的存储器结构
  * 二进制逻辑+程序存储执行+五大部件（运算器、控制器、存储器、输入设备、输出设备）

**原语（**Atomic Operation）：最接近硬件；原子性；运行时间短且调用频繁。

定义原语的方法是关闭中断。

互斥锁：c++中的 \<mutex\> 做到对某些数据的互斥访问 (non-recursive，不能用在递归函数中)

```c++
std::lock_guard<std::mutex> guard(g_mutex);
g_var = new_value;
// release `mutex` by the constructor of guard
```

条件变量 c++中的 \<condition_variable\> 做到线程之间同步

```c++
std::condition_variable g_cv;
std::mutex g_mutex;
bool ready = false;
bool processed = false;

void thread_work(){
	std::unique_lock<std::mutex> lk(g_mutex);
	cv.wait(g_mutex, []{return ret_value;})
	...
    processed = true;
	lk.unlock();
	g_cv.notify_one();
}

void main()
{
	std::thread worker(thread_work);
	// send data
	{
		std::lock_guard<std::mutex> lk(g_mutex);
        ready = true;
        
		
	}
    cv.notify_one();
    {
        std::unique_lock<std::mutex> lk(g_mutex);
        g_cv.wait(lk, []{return processed;});
    }
    worker,join();
    return 0;
}
```

 

信号量 semaphore /seme'fc'(r)/允许N的线程同时访问临界区 

c 中的 <semaphore.h> , c++11没有提供但可以用上面两个的组合来实现（因为容易犯错？）

```c++
int sem_destroy(sem_t *sem); 
int sem_wait(sem_t *sem);
int sem_trywait(sem_t *sem);  
int sem_post(sem_t *sem);
int sem_getvalue(sem_t *sem); 
```

c++的标准原子类型 \<atomic\> 实现了诸如此类的操作：（实现原子的自增）

```c++
std::atomic<int> x = 0;
std::atomic<int>::fetch_add 
```



**进程**

* **PCB**
  * 标识信息
  * 状态信息
    * 用户可见寄存器
    * 控制、状态寄存器
    * 栈指针
  * 控制信息
    * 调度信息：进程状态、优先级、时间
    * IPC的标记、信号等信息
    * 内存分配信息
* **处理器执行模式**：为了保护OS和OS的控制表（如PCB表）不受用户程序干扰
* **中断机制**
  * 来自当前执行指令的外部，中断
  * 与当前执行指令相关，陷阱
  * 系统调用
* **OS系统的执行**
  * 传统且通用的无进程内核
  * 在用户进程内进行：OS的代码和数据位于共享区域内被所有用户进程共享
  * 基于进程的OS：把OS的不同功能用简单独立的进程实现，如监视器



**线程**

* **线程与进程的区别、**
  
  进程两个 **独立**的特点：资源的所有权（进程映像）、调度执行，把这两点分离开来，把进程分为一组相关联的执行单元，也就是线程：
  
  进程是资源分配单元，线程是调度单元
  
  * 线程独占：PC，栈，寄存器，用于局部变量的静态存储空间
  * 线程共享：文件，代码，共享数据，堆（线程在堆上分配内存时需要同步原语）
  
  进程中，所有线程**共享该进程的状态和资源**。线程创建、切换的**开销更小**。**无需内核介入的程序间通信**，更简单且效率高，举例：
  
  * 前后台工作：一个线程负责读取用户输入，一个负责处理输入，更新表格等
  * 异步处理：文字编辑器，专门的一个线程直接由OS调度，负责周期性地把内存缓冲区的数据写入磁盘
  * 执行速度：一部分线程负责从设备读取，另一部分负责计算处理，即使读取阻塞，仍可运行
  * 模块化程序结构
  
* **线程同步**：对进程内共享数据的写操作，采用的技术和进程间同步相同。

* 线程分类

  * 用户级线程（ULT)：完全由用户程序来管理
  * 内核级线程 (KLT)

* 线程调度与进程调度

  * **纯ULT的优点**：线程切换不需要内核模式特权，（省了两次状态切换开销）、为应用程序量身定做调度算法、忽略OS层次的细节，可在不同OS下运行，由线程库提供一组应用级函数
  * **纯ULT的缺点（相较于KLT）**：
    * 纯ULT策略中，一个系统调用就会**阻塞该进程的所有线程**。（可以采用jacketing技术，把产生阻塞的系统调用转化为非阻塞系统调用：如，让线程调用一个应用级的IO jacketing例程，这个例程中的代码负责检查IO是否在忙，忙就让该线程进入阻塞，把控制权交给另一个线程。）
    * **进程一次最多被分配一个处理器**，不能应用多处理技术。（可以把程序写成多进程程序，但开销就大了）
  * 切换开销：ULT < KLT < Process
  * 混合ULT、KLT
  * 线程与进程的数量对应关系，可以任意（1：1如传统UNIX、M:1如win和LInux，还有1：M线程在不同进程环境中迁移如，分布式OS中，M:N结合了前两者）

**概念区分：**

* 进程（process）：资源管理单位
* 线程（thread）调度单位
  * 上下文切换时抢占式多任务处理（内核态）
* 纤程（fiber）：UTL
  * 上下文切换，是合作式多任务处理（用户态）
* 协程（coroutine）：程序语言提供的纤(xian)程支持

**调度**

**单核调度**

* 长期调度：是否将其纳入进程管理
* 中期调度：换页机制的一部分，考虑内存开销
* 短期调度：实际决策

* FCFS：先到先服务（非抢占）
* SJF：最短任务优先（非抢占）
* STCF：最短完成事件优先（抢占式）
  * 降低周转时间
* RR：时间片轮转
  * 降低任务响应时间，适合交互式
* 优先级调度
* MLQ：多级队列
* MLFQ：多级反馈队列：动态设置优先级
* FSS：公平共享调度
  * 彩票调度
    * 彩票转让：通过转让份额，让持有锁但份额小的进程能够执行
    * 彩票货币：自定义子任务的份额分配
    * 彩票膨胀：让子任务根据自己的需求决定自己的份额
  * 步幅调度
* RTS：实时调度

**多核调度**

* 负载分担（load sharing）：把多核规约到单核调度问题
  * 简单、CPU不会浪费
  * 但同步开销大
  * 同一任务在不同处理器上切换的开销（可用两级调度解决）
* 协同调度：尽可能让一组任务同时执行，并避免同时调度依赖任务（任务A需要任务B完成才可执行），避免关联任务（任务A、B需要边通信边执行）的执行效率低问题
  * 适用于并行计算场景，即任务被切成多个子任务并行执行。该场景中：
    * 整体同步并行（BSP）计算模型与协同调度非常契合：并发计算（每个核心独立计算自己的任务） + 通信（核心之间） + 同步（到达屏障点后需等待其他核心也到这个屏障点）
  * 群组调度，把关联任务设置为一组，以组为单位在多核心上调度
* 两级调度：全局+本地（每个核心有一个）
  * 负载会不均衡
    * 如何确定当前任务的负载情况？Linux采用的是PELT(调度实体粒度负载追踪)：设定一个周期，记录每个任务在该周期内running或ready的时间x，该任务的负载就是x/T，把所有任务的负载累加起来（直接累加、或者指数衰减、多项式衰减，（时间太久远的意义不大））得到当前的负载。PELT开销较小。
    * 要考虑负载均衡策略的开销
    * 不同的核心之间不是对等的。如NUMA(非一致性内存访问)中，多个核心组成一个节点，拥有一个本地内存，访问速度较快，但访问其他节点的远端内存就比较慢。节点之间的任务迁移开销会很大。
    * 任务在同属于一个物理核（可分为多个逻辑核）的不同逻辑核的迁移开销较小等
  * Linux的负载均衡（层级调度）
  
    * 调度域：拥有相同特性的CPU集合，这些核心见可以进行负载均衡
    * 调度组：一个调度域保存一个或多个调度组。调度组是一个调度域内进行负载均衡的整体单位。
    * 自下向上的方式进行负载均衡
      * 越高层级间进行负载均衡，开销越大，所以Linux在不同层级的调度域设置了不同的负载均衡出发频率和阈值（只有当两个调度域的负载之差大于阈值才触发）。显然每一层的参数设置不尽相同。                                                                                     

根据任务特性，划分为：（不同的任务的调度策略不尽相同）

* 批处理
* 交互式
* 实时任务

Linux提供多种调度策略和调度器，来适合不同的应用场景：

* CFS(完全公平调度器)：针对优先级低的后台任务
* RTS(实时调度器)：执行直至结束或者被抢占 、 执行一定时间片后不在执行
* DLS(截止时间调度器)：

Linux 2.6.0 的O(1)带调度器，采用了两级调度的思想。每个CPU核心单独维护一个本地运行队列，让任务仅在同一个CPU核心上调度。 每个本地运行队列= 激活队列 （active，有实时间片剩余）+ 过期队列 （expired，时间片耗尽）。当激活队列为空时，两个队列互换，开始新一轮的调度。可以利用位图表示该多级队列的哪些队列是非空的。交互式任务不希望等待所有任务的时间片用完重新调度，所以需要复杂的交互式任务的判定算法

Linux 2.6.23 为了解决上述问题，使用了完全公平调度器（动态时间片，RB树作为运行队列按`vruntime`排序，阻塞任务唤醒）



**概念区分**

* 宏内核：，也叫单内核，OS内核的所有模块（进程调度、内存管理、文件系统、设备驱动）均运行在内核态，具备直接操作硬件的能力，如Unix/Linux，FreeBSD
* 微内核：服务与服务之间隔离，

**宏内核** **进程间通信**（IPC)

按**方向**分类：

* 仅单向
* 仅双向
* 单+双

按**时序**分类

* 同步
  * 会阻塞进程
* 异步
  * 非阻塞，采用内核缓冲区，避免等待

超时机制：防止被调用者恶意不回复消息，或者太忙而丢失了消息。问题是难以确定超时阈值，解决方案是引入两个特殊的超时选择：

* 永不返回：类似于没有超时机制，也就是超时阈值设置为无限大，更侧重功能性
* 立即返回：只有被调用者处于可以立即响应的状态才建立通信，更注重安全性（避免了拒绝服务攻击等）

按**方式**分类

* 直接：需要显式的识别另一方。如信号
* 间接：通过中间的信箱，每个信箱有自己的唯一标识符，如管道

OK，下面是几种常见的通信方式：

* 基于共享内存的消息传递
  * 以共享内存为媒介
* 共享内存
  * 直接在共享内存建立数据结构。因为共享内存在不同进程的虚拟地址空间中可能是不同的，导致指针与指针相关的数据无法使用。此外他还要求进程彼此之间信任，打破了进程间的隔离性优势。
* OS内核辅助的消息传递：
  * 使用控制流转移可以避免轮询，减少CPU资源的浪费

|               |            |            |      |            |              |                                                |                                             |
| ------------- | ---------- | ---------- | ---- | ---------- | ------------ | ---------------------------------------------- | ------------------------------------------- |
| pipe          | 单向       | 一般是同步 | 间接 | 字节流     | 两个进程间的 | FIFO缓冲区管理                                 | 匿名、命名管道                              |
| message queue | 单向、双向 | 一般是异步 | 间接 | 消息为单位 | 多个进程之间 | 队列管理（文件权限）                           |                                             |
| semaphore     | 单向、双向 | 一般为同步 | 间接 | 计数器     | 多个进程之间 | 内核维护共享计数器                             | P , V操作， 原子操作                        |
| share memory  | 单向、双向 |            | 间接 | 内存区间   | 多个进程     | 内核维护内存区间                               | 速度快，性能好                              |
| signal        | 单向       | 一般是同步 | 直接 | 事件编号   | 多个进程之间 | 为线程、进程维护信号等待队列（组/用户权限）    | 单向事件通知，接收进程不需要主动的等待/阻塞 |
| socket        | 单向、双向 |            | 直接 | 数据报文   | 两个进程之间 | 基于IP端口和文件路径的寻址方式，利用网络栈管理 | 本地+网络通信                               |



* 管道
  
  * 位于内存的文件，创建后返回两个fd，对应写端口和读端口。
  * 管道为空时如果没有进程拥有写端口，读端口会收到EOF，否则读端口会阻塞（也可配置非阻塞选项）
  * 管道为满时，写端口会阻塞
  * 匿名管道：没有全局名称，只有两个fd。比如父进程创建一个匿名管道拿到两个fd，在fork一个子进程，子进程也会拥有这两个fd,然后二者再根据需要关闭各自的其中一个fd，即可进行通信。这不适合关系远的进程
  * 命名管道，通过命令`mkfifo`创建，指定一个全局文件名，以此来指代具体的管道。
  
* 消息队列
  
  * 结构是链表：链表头（消息队列的信息如权限，最大空间，当前消息个数）+ 包含消息（类型+数据字节流）的节点
  * 四个基本操作
    * `msgget`：允许进程获取一个消息队列的连接，或创建一个新的消息队列
    * `msgsnd`：进程往消息队列发送消息，满时默认阻塞
    * `msgrcv`：进程从消息队列接收消息，无消息时默认阻塞
    * `msgctl`：管理消息队列，如修改权限信息或删除消息队列
  * 发送时，会从用户态拷贝到内核态；接收时会从内核态拷贝到用户态。因此不适合长消息（共享内存更适合）
  
* 信号量

* 共享内存
  
  * 内核为全局的共享内存维护一个全局的队列结构
  * 进程如何知道哪一段虚拟地址映射的是共享内存区域呢？通过为进程分配的`VMA`结构体，让他们都指向`file`（封装共享内存的文件结构体），不想用了就单方面取消映射
  
* 信号，如Ctrl+C

  * 一些特定的信号会有对于的处理函数，当进程接收到信号时，内核会自动将该用户的控制流切换到处理函数上，

  * 如果一个进程接收到多个相同信号，内核只记录一次。但实时信号的重复信号是不可忽略的，后续由POSIX引入了实时信号（32-64），之前的叫常规信号（1~31）。

  * Linux为每个进程和每个线程都准备一个信号事件等待对列，同进程的线程共享该进程的信号事件等待队列。

  * 通过`sigprocmask`系统调用。来屏蔽指定信号的处理。但进程接触屏蔽后，有可能要处理被添加到队列上的之前被阻塞的信号。一些重要信号不能被阻塞，如杀死进程。

  * 内核对信号的处理：

    * 忽略
    * 调用用户注册的信号处理函数
    * 调用默认处理函数（如无用户注册函数）：大多数是忽略或杀死进程

  * 用户处理函数的可重入：

    处理信号的过程是。用户调用系统调用，进入内核态，内核态返回用户态时检查是否是有信号处理，如果有，则把当前上下文保存到用户栈，再进入用户态执行用户注册的信号处理函数。处理完，调用系统调用`sigtreturn`进入内核态，在内核态执行`sigreturn`从用户栈上恢复上下文，再次检查信号，如无，返回用户态。

    如果在用户态执行用户注册信号处理函数时，发生了中断，进入内核态，在返回用户态时，又检测到信号，导致旧信号没处理完你，又开始处理新的信号。如果两个信号处理过程中对同一静态数据读写或同一全局锁的请求，会造成结果 的不一致、或死锁。

    这要求，我们编写信号处理函数时要注意可重入性：

    * 不使用静态数据，或者只是用只读的静态数据
    * 尽量用本地数据
    * 对全局数据加以保护，（但也要注意死锁
    * 不调用不可重入的函数，如`malloc`

* 套接字

  * 方式1：IP寻址方式的本地通信使用 127.0.0.1 回环地址
  * 方式2：使用本地文件系统的一个路径
  * 通信方式 （`type`） 数据流`SOCK_STREAM`（TCP）、数据报`**SOCK_DGRAM`（UDP）**

**微内核  进程通信**

### **同步**

**临界区**

* 单核可以通过关闭中断，防止抢占来做到
* 多核有皮特森算法：经典皮特森适用两个线程，可以拓展到任意个线程
* 现代CPU为了更好地性能允许访存操作乱序执行，先假定没有乱序执行。

**两种互斥锁**：解决临界区问题

* 自旋锁，基于原子操作CAS(Compare and Swap)，只有0和1两个值
* 排号自旋锁，FIFO的更公平，基于原子操作FAA（Fetch and Add），维护一个累加器和当前值`turn`

**条件变量** ：提供挂起/唤醒机制来避免循环等待，节省CPU资源，需要与互斥锁搭配使用

一种实现

```c
struct cond {
	struct thread *wait_list;
};

void cond_wait(struct cond *cond, struct lck *mutex) {
	list_append(cond->wait_list, thread_self());
	atomic_block_unlock(mutex); // block和unlock必须是原子的
	lock(mutex);
}

void cond_signal(struct cond *cond) {
	if(!list_empty(cond->wait_list))  // 把if改成while就是cond_broadcast进行广播唤醒所有阻塞的线程
		wakeup(list_remove(cond->wait_list));
}

```

**信号量**

* 二元信号量，只有0,1，类似于互斥锁，区别在于：
  * 互斥锁有，拥有者，这一概念，（在语义上是如此要求的）往往是同一个进程加锁，放锁。（多个线程对**一个**共享资源的访问
  * 信号量允许不同的进程执行`wait`和`signal`操作。（多个线程对**一系列**共享资源的**有序**操作。
* 信号量由：互斥锁、条件变量、计算器共同实现，实现了更高层级的抽象

**读写锁**：

* 偏向读者，有很高的读者并行度，适合读多写少
* 偏向写者，在读者离开临界区后，写者马上进入临界区
* FIFS

**RCU**（Read-Copy Update） ，读写锁允许多个读者同时进入临界区，但位于临界区的写者会阻塞读者。RCU则是不阻塞读者（**读写互不阻塞**），读者要么读取到旧的值，要么读取新的值，而不会读取到一个中间值（非法/无效值），通过**订阅/发布**机制来保证这一点。

* 订阅：对数据节点的读取封装
* 发布：对数据节点的更新封装：当数据节点初始化完成，才设置原链表的`Next`指针域，不会被读取到中间值。这样可以原子地写任意大小的值。
* 封装的原因是避免CPU乱序执行和编译器的优化，因此打包成一个接口方便开发者使用

原理是，通过对数据节点的指针更新（对64bit的指针更新可以由硬件保证是原子的），读取链表时要么的读取到节点的旧`Next`要么是是新的`Next`。

思考以下，插入节点、删除节点、更新节点（提示：先copy）如何分别保证原子。

带来的问题：如何回收某一数据节点，即如何确定有没有读者正在读该节点？

* 通过宽限期。写者更新完`Next`后，观察最后一个可能读取到旧值的读者，在其结束临界区后回收掉被删除的数据节点。读者通过互斥锁来记录临界区的开始的结束。

**管程（Monitor）**：保证同一时刻最多只有一个操作者进入管程的保护区域访问共享数据。是对共享数据和操作共享数据的函数的封装。是高层级抽象，如Java中的`synchronized`

### **死锁**

* 死锁
  * 发生的**必要**条件
    * 互斥
    * 持有等待··
    * 不可抢占
    * 循环等待
  
  
  
  **可重入锁**指的是加锁判断时会判断锁的持有者是否是该线程本身，如果是就计数器+1不阻塞。如中断再中断，递归函数中等。
  
  * 解决方案：
    * 严格预防：避免互斥访问（代理线程专门来管理共享内存，代理太多系统负担大）、不允许持有等待（一次获得所有，会陷入申请-释放的循环被饿死，或者陷入活锁），允许资源抢占（被抢占的线程难以恢复），避免循环等待（对资源编号，采取有序算法，如递增获取，系统总有一个得到最大号的资源的线程，该线程可以持续运行，但不够灵活）
    * 假装没有
    * 银行家算法（死锁避免）、检测环（死锁检测开销大）
  
  **活锁：**没有阻塞，但陷入了“尝试-失败-尝试-失败”的循环。线程1、2都需要获取资源A、B。1得到A的同时2得到B，然后两者均释放自己的锁，两者执行速度相同，又陷入循环。
  
  **优先级反转**：指的是高优先级线程在等待被低优先级线程持有的锁时，该低优先级线程被中优先级线程阻塞，从而造成了高优先级线程间接地被中优先级线程阻塞。这对于实时系统来讲后果很严重。高优先级进程可能因此错过deadline。
  
  * 解决方案：不可抢占临界区协议(NCP)。但这样，所有线程都会被持有锁的低优先级线程阻塞，即使是毫无关系的高优先级线程，这会带来相同的问题。更好的方式是：
  * 优先级继承协议（PIP）：高优先级继承锁时，持有锁的低优先级进程会继承高优先级进程的优先级  如果T3继承了T1的优先级后，更高优先线程T0又来竞争同一把锁， 获得锁的顺序 T3  -> T1 -> T0
  * 优先级置顶协议（PCP）：把获得锁的线程预先设置为可能竞争该锁的最高优先级
    * 即时优先级置顶协议（OPCP）：提前设置
    * 原生优先级置顶协议（IPCP）：出现了竞争再设置  如果T3继承了T1的优先级后，更高优先线程T0又来竞争同一把锁， 获得锁的顺序 T3 -> T0 -> T1
  
  chCore的多核加锁方案是：使用一个大内核锁，在所有进入内核的地方加锁，在所有出内核的地方释放锁。同一时刻，只有一个核心可对内核中的对象就行修改。
  
* 事务ACID
  * 原子性（atomicity，或称不可分割性）要么全部完成，要么全部不完成
  * 一致性（consistency）在事务开始之前和事务结束以后，数据库的完整性没有被破坏
  * 隔离性（isolation，又称独立性）隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致
  * 持久性（durability）事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。

### 内存管理

一个进程一个段表，                                      

已被分配的虚拟内存不一定映射到物理内存，当发生缺页时，采用了换页机制，在内存与磁盘间

Linux中通过VMA来 区分未分配 和  已分配但未映射到物理区域。

**换页策略**

* FIFO
* OPT
* LRU
* MRU
* Clock
* Working Set

**虚拟内存**

* 实现共享内存
* 写时拷贝
* 内存去重，但当攻击者知道了有重复数据也就说明这些数据很敏感

**大页**

* 减少对TLB表项的占用
* 加快多级页表的翻译
* 资源浪费
* 复杂高

**伙伴系统**

每个块大小都是2的幂次，大块可以分裂成两个等大的块，称为伙伴块。两个伙伴块可以合并成一个大块。（互为伙伴的两个块，仅有一位物理地址不同，可以高效地管理内存）

* 请求分配m个页：查找 $2^{n-1}<m\leq 2^n$，如果全局数组索引为n指向的链表为空，就去索引为n+1的找，直至找到，找到后如果可分裂，就分裂，其中一个作为服务块，另一个插入对应索引的链表中表示空闲待用。
* 释放块时：找到该块的伙伴块，如果伙伴块空闲就合成大块，再按照释放该大块的操作继续处理；如果非空闲，就把该块插入到对应索引项指向的链表中。

**SALB**分配器：解决伙伴系统中小内存的分配问题（如几十几百字节，远小于物理页大小），把伙伴系统分配的大块内存进一步细分为小块进行管理。 

**文件系统**

| 文件类型     |                                  |                             |
| ------------ | -------------------------------- | --------------------------- |
| 常规文件     | 保存数据                         |                             |
| 目录文件     | 组织一组文件                     | 记录了文件名到inode号的映射 |
| 符号链接文件 | 保存符号链接（指向目标文件的路径 |                             |
| FIFO文件     | 即管道                           |                             |
| 套接字文件   | 用于传递数据，比管道灵活         |                             |
| 字符设备文件 | 表示和访问字符设备               |                             |
| 块设备文件   | 表示和访问块设备                 |                             |

**硬链接**

文件名不是文件的元数据，一个文件可以有多个文件名，对文件的任一硬链接进行写操作时，会影响到其他硬链接。只有当所有硬链接都删除时，该文件才会被删除（即inode及其数据才会被删除，通过`nlink`指明被链接了几次）

目标文件不可为目录

> ​	创建新的目录文件时，其本身连接数为2（.和..）其父目录链接数+1。创建新文件时，父目录链接数+1

**软链接（符号链接）**

符号链接文件保存一个字符串，表示一个文件路径，直接保存在其inode的原本保存数据块指针的空间中。删除符号链接文件对原文件无影响，符号链接文件可以保存不存在的目标文件路径，解析时会报错。**不受文件系统边界的限制**

## 设备管理

**设备与CPU连接**的方式是：Bus。如AMBA，PCIe等。CPU通常通过读写设备寄存器来和设备通信，如MMIO，PMIO。分别通过访存指令和端口指令和设备进行交互

DMA,直接内存访问，让设备绕过CPU直接访问内存，提高CPU 的利用率。发起者可以是CPU也可以是设备

> ​	发起者是CPU，设备驱动首先在内存分配一块DMA缓冲区，随后发起DMA请求，设备收到请求后通过DMA
>
> 机制将数据传输至DMA缓冲区。DMA操作完成后，设备触发中断通知CPU对DMA缓冲区的数据进行处理
>
> * 处理器向DMA控制器发送DMA缓冲区的长度和位置，以及数据传输方向，随后放弃对总线的控制
> * DMA控制器获得总线控制权，可直接与内存进行通信
> * DMA控制器根据从处理器获得的指令，将设备的数据拷贝至内存，在这期间处理器可以做其他事
> * DMA控制器完成DMA后向处理器发送中断，通知处理器DMA已经完成。此时，处理器重新获得对总线的控制权

**设备识别** 的方式有设备树、ACPI (高级配置与电源接口)

**设备中断**

在GIC(通用中断处理器)中，分为

* 软件生成中断，SGI
  * 0~15， 核间通信等产生，需交给指定的核处理，每个核各自维护中断队列
* 私有设备中断，PPI
  * 16~31， 1056~1119，设备产生，需交给指定核处理，每个核各自维护中断队列
* 共享设备中断，SPI                             
  * 21~1019, 4096~5119, 设备产生，交给指定或非指定的任意一个或多个核处理，维护一个共享的中断队列

 中断响应过程

* Generate：中断源产生中断，传给GIC，中断从Inactive变为Pending
* Deliver：GIC将中断传给CPU
* Activate：CPU调用中断处理函数响应并处理该中断，中断处于Active
* Deactivate：CPU处理完，通知GIC处理完毕，GIC将中断更新为Inactive （也叫EOI阶段，只有CPU确认了EOI后，对应的中断才能重新被响应）
<style class="fallback">body{visibility:hidden}</style><script>markdeepOptions={tocStyle:'long'};</script><!-- Markdeep: --><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>